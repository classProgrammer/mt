\section{Approach} % what's included/excluded and why
% problem analysis
The first step in the design process of a chatbot is the problem analysis.
As mentioned in \citet{folstad2017chatbots}, the design focus of chatbots lies in the conversation.
The analysis of the example conversation led to the identification of the intents and entities required for the use-cases.
Figure \ref{fig:sicknessflow} shows the example conversation and Table \ref{tab:conversation_data} the analysis.
Figure \ref{fig:conversationflow} shows the resulting flow-chart for the use-cases.
The flow-chart is the blueprint for the chatbot prototype development.
This ensured that the dialog structure is the same for all bots making a fair comparison possible.

% suitable problem
The problem proofed to be solvable with chatbots based on the problem analysis.
As mentioned before, three criteria decide if the problem is suitable or not.
The flow-chart clearly shows that both use-cases are solvable through back and forth communication, making the first statement valid.
The flow-chart also shows that both use-cases are repetitive data collection tasks.
The chatbot needs to identify the type of user message that can either be a sickness notification or vacation request.
Then the required data is extracted from the message.
For the sickness notification, the required data is a name, and the optional parameter is the return date.
The required parameters for the vacation request are the name of the person and a date-span or a start and end date.
Hence, the second statement of \citet{singhbuilding} is also valid since the problem is repetitive.
The communication is solely between the user and the bot without the interaction of a third party.
This means that the problem is automatable since no interaction with a third party is necessary.
To summarize, all three statements are valid for the given problem, and the use-cases are suitable for chatbots.

% key functionality of chatbots
As mentioned in Chapter \ref{chap:basics} Basics, the classification of sentences and extraction of data are the two key features chatbots have to offer.

% task-oriented chatbots
The given problem is task-oriented and tied to a domain.
The three major chatbot approaches are task-oriented, menu-driven, and open-ended. 
\citet{deshpande2017survey, luis2015williams, braunEvaluatingNLU, williams2017hybrid} use and describe task-oriented approaches.
Task-oriented chatbots offer limited, domain-specific functionality. 
Questions outside of the domain are out of scope and not relevant.
The chatbot needs to extract specific data from the sentences, which is an indicator that the problem is task-oriented.
The second indicator that the problem is task-oriented is that the chatbot has to collect information from the conversation to achieve a specific task.
The chatbot has to be able to answer questions in the domain, which consists of sickness notifications and vacation requests.
It is not required to answer any questions that are unrelated to the given domain.
A limited set of operations also fits the requirements.
To conclude, the task-oriented approach is a perfect match since the description meets all requirements.

% menu-driven chatbots
The given problem can be solved theoretically by the menu-driven approach, which is not as suitable as the task-oriented approach in general.
The given problem requires a mechanism to input dates and person names in full and partial sentences to provide a natural user experience.
The menu-driven approach uses numbers to navigate a user through a menu.
The number one could be used for sickness notifications and two for vacation requests. 
Then the user inputs the name and dates and confirms the information.
To summarize, it is possible to use the menu-driven approach, but it would neither be ideal nor pleasant.
The flexibility of the menu-driven approach is also limited compared to the task-oriented one, which is a negative trait when further projects are likely.

% open-ended chatbots
The third major chatbot type found in literature is the open-ended approach used in \citet{williams2017hybrid, bordes2016learning}, and \citet{rahman2017programming}.
The open-ended approach has no specific goal, is not limited to a domain, and has problems with collecting specific data.
The collection of data to achieve a task is crucial to implement the use cases, which is an indicator that this approach is unsuitable.
Furthermore, as \citet{bordes2016learning} mentioned, the open-ended approach is not yet able to perform well enough in task-oriented settings to be considered for this thesis.
The analysis showed that both use-cases are task-oriented.
This is the second indication that this approach is not the right one.

% hybrid
\citet{williams2017hybrid} combined the open-ended and task-oriented approaches by adding state-related information to and open-ended system.
The results showed that the hybrid approach works better than the classic open-ended approach, but the comparison with task-oriented technologies is missing.
Hence, it is impossible to tell if a hybrid approach makes sense for a task-oriented problem or not.
Open-ended approaches usually perform terribly in task-oriented settings, which makes the statement of \citet{williams2017hybrid} irrelevant since better than terrible is not precise.
To summarize, the task-oriented approach is a perfect fit for the requirements, while the open-ended approach proved unsuitably.
The hybrid approach is not used since the comparison with a task-oriented approach was not provided.
The menu-driven approach could be used but is not as pleasing as the task-oriented one.

% domain
The domain for this thesis is sickness notifications and vacation requests.
The prototypes only provide domain-relevant functionality since further functionality is not required.
The functionality only covers the two request types sickness and vacation and basic structures like hello, bye, thank you, yes, and no to build the conversations.

% framework selection
Dialogflow, LUIS, Rasa, and Watson Assistant were selected for prototyping.
In general, lots of chatbot frameworks are available.
As mentioned by \citet{kane2016role}, the two major categories for chatbot frameworks are cloud-based solutions developed via a website and local standalone applications.
The framework selection for this thesis was based on chapter \ref{chap:soa} State-of-the-Art and Section \ref{sec:prereq} Prerequisites.
Section \ref{sec:prereq} describes the requirements of the company, and Chapter \ref{chap:soa} shows the technologies present in papers, articles, and books.
To match the prerequisites at least one cloud (\citet{braunEvaluatingNLU, rahman2017programming}) and one local (\citet{braunEvaluatingNLU}) chatbot are needed.
IBMs Watson Assistant (\citet{rahman2017programming, pharmacybot, ieee2018watson, gregori2017evaluation}) is inevitable because it is on the wish-list of the company.
A common cloud technology is Dialogflow (\citet{braunEvaluatingNLU, dutta2017developing, singhbuilding, buiildChatbotsPython, rahman2017programming, ieee2018watson}) hence it is selected.
A common local standalone technology is Rasa (\citet{braunEvaluatingNLU, singhbuilding, rasabocklisch2017, buiildChatbotsPython, gregori2017evaluation}) and is the local tool of choice.
The chosen chatbot technologies were Dialogflow, IBM Watson, and Rasa.
Additionally, LUIS\cite{luis2015williams} was added for the NLU comparison.
It was the chatbot technology with the best NLU performance in \citet{braunEvaluatingNLU}.
LUIS has been mentioned or was used by \citet{singhbuilding, buiildChatbotsPython, rahman2017programming, dutta2017developing}, 
and \citet{gregori2017evaluation}.

% training data
All prototypes use the same training data.
Figure \ref{fig:conversationflow} shows the dialog structure, which was implemented with all technologies to ensure a fair comparison.
Figure \ref{fig:sicknessflow} shows an example of a conversation of the sickness notification task. 
The NLU part of all four technologies has also been trained with the same training data to ensure a fair comparison with the same initial state.
The data used for training has been listed in
Table \ref{tab:sickness_utterances}, \ref{tab:vacation_utterances}, \ref{tab:sickness_utterances_de}, 
and \ref{tab:vacation_utterances_de}.
The same training data also means that all technologies have the same intents, entities, and utterances.
The training data for the prototypes has been defined by the author which is a valid approach for prototyping.
\begin{table}[h]
    \centering
    \begin{tabular}{ c | l | c | c   }
        \multirow{2}{*}{No} & \multirow{2}{*}{Utterance} & Person & Date \\ 
                 &&         Entity & Entity                 \\ \hline \hline
        1 & I am sick & \xmark & \xmark\\ \hline 
        2 & sickness notification & \xmark & \xmark\\ \hline 
        3 & I am ill & \xmark & \xmark\\ \hline 
        4 & [Alfred Mayer] is sick & \cmark & \xmark\\ \hline 
        5 & my colleague [Maria M\"uller] is sick until [wednesday] & \cmark & \cmark\\ \hline 
        6 & colleague [Simone Bauer] is ill & \cmark & \xmark\\ \hline 
        7 & report [Franz Dorfer] sick & \cmark & \xmark\\ \hline 
        8 & [Stefan Weber] is ill till [Friday] & \cmark & \cmark\\ \hline 
        9 & sick [Emma Wagner] [6th of June] & \cmark & \cmark\\ \hline 
        10 & [Sophia Richter] feels sick today & \cmark & \xmark\\ \hline 
    \end{tabular}
    \caption{Sickness Training Utterances English} \label{tab:sickness_utterances}
\end{table} \noindent

% entities
It was impossible to use the same entities since they are different for each technology.
It was necessary to create an entity from scratch for Watson Assistant and LUIS (in German) since no predefined person entity was available.
In Engish, Dialogflow and LUIS provided all required entities by default. 
Rasa needed external tools to provide the entities.
The missing entities were added to the frameworks to make the NLU performance comparison as fair as possible.
The same training data for all technologies approach has been used 
by \citet{braunEvaluatingNLU, dutta2017developing} and \citet{gregori2017evaluation} and was adopted for this thesis to enable a fair comparison.

% tests
The two primary functionalities of chatbots have been tested inside the given domain to answer which chatbot technology is the best to solve the given problem.
The primary functions of chatbots are the intent classification and entity extraction.
The entity extraction capabilities were tested by \citet{geyer2016named}.
The intent classification and entity extraction was evaluated in \citet{braunEvaluatingNLU} using precision, recall, and f-score values.
The precision, recall, and f-score values are the base values for the evaluation of the entity extraction and intent classification capabilities.
This approach provides an objective numeric value for a chatbot system's performance and prevents subjective bypass for tests.
The intent classification data used for all technologies has been listed in 
Table \ref{tab:sickness_intent_classification},
\ref{tab:vacation_intent_classification}, \ref{tab:sickness_intent_classification_de} and \ref{tab:vacation_intent_classification_de}.
The entity extraction test data has been listed in Table \ref{tab:person_entity_extraction_recognition}, 
\ref{tab:date_entity_extraction_recognition}, \ref{tab:date_entity_extraction_recognition2}, 
\ref{tab:date_span_entity_extraction_recognition}, and Table \ref{tab:date_span_entity_extraction_recognition2}.
The test evaluation focuses solely on domain-specific information since the primary purpose of this thesis is a proof of concept through prototyping.

% domain test influence
The result of \citet{braunEvaluatingNLU} showed that the domain had small to no influence on the performance of the tested technologies.
If the results can be trusted, good technologies will perform excellently no matter if the environment is domain-specific or not.
A lousy technology will perform poorly in any setting.
As \citet{braunEvaluatingNLU} said, different technologies should be tested with domain-specific data to determine which is the best fit for the given problem.
The domain-specific performance is the relevant performance since it has a big influence on the product.
A chatbot technology that performs great in general but poorly in the given domain is of no value.
All in all, The best technology is used to implement the actual product and not the technology that performs best in general domain-independent cases.
The prototypes of this thesis were implemented and tested solely with domain-specific training and test data.

% user study
As mentioned by \citet{evaluateChatbotsShawar2007}, user studies are another common approach to test chatbot systems.
They are recommended for customer-facing bots to increase the quality of the product and gather data about the human bot interaction.
User-studies are not part of this thesis.
A proof of concept through prototyping user studies are not required.
Furthermore, a user study does not provide any information about the development perspective, which is a vital part of the thesis.

% psychological aspects
The psychological aspects mentioned by \citet{folstad2017chatbots, brandtzaeg2018chatbots} and \citet{GO2019304} have been partially applied.
The general rule that a chatbot should tell the user that he is talking to a bot was applied.
For further projects, the humanness of bots is an important factor and should be evaluated and implemented.
The humanness is an exciting topic for user-studies and can significantly increase user experience and acceptance.
Standard approaches are giving the bot a human name, gender, and face.
Amazons Alexa is one example of a bot with a human name, gender, and a voice.
However, it is not relevant for the proof of concept because there are no user studies in this thesis.


\section{Comparison and Evaluation}\label{sec:comparison_and_eval}
% test setup: data 
A major part of this thesis has focused on the evaluation and comparison of the developed prototypes.
The training and test data used for the comparison are domain-specific. 
The evaluation of the entity extraction and intent classification capabilities of the framework relies on the provided data sets.

% entity extraction
It was impossible to test the entity extraction capabilities of the frameworks unbiassed.
The person entity is unavailable for Watson Assistant, and LUIS had none for the German language.
The bias came from the creation of the entities since the training data quality is of different quality compared to the predefined ones.
An entity created by the developer might overfit or suffer from a small amount of training data.
This is a bias in the test setting since the performance is dependent on the training and test data provided by the developer and not on the technology alone.
With Watson Assistant, it was not possible to mark entities in the training sentences for the German language.
This rendered Watson Assistant useless for the German input language.
Watson Assistant provides no date span entity.
Two separate date entities were used instead to get the same result.
  
The Rasa framework does not offer any predefined entities itself and relies on other optional technologies like Spacy and Duckling.
Spacy offers date and date-span entities, but the result date is not in a standardized format.
The resulting dates are not machine-processable.
The automation criterion of chatbots requires dates in a standardized form.
To summarize, the date and date span entities of spacy were unsuitable.
This lead to the use of Duckling since it provided the necessary standardization for date and date-span entities out of the box.
Dialogflow offered two entities that can handle date-spans.
Both entities were tested to get the best result possible. 

The entity extraction evaluation uses the recall, precision, and f-score values for the comparison and the ranking, as done by \citet{braunEvaluatingNLU}.
The result of each test case is either TP, FP, FN, or TN.
The entities defined by the problem analysis of the user-cases are the person, date, and date-span entity.
The entity extraction test only included these three entities to get a result relevant to the use-cases.
Furthermore, the calculation in groups showed the performance for specific entities and intents. 
This allowed a detailed performance comparison for the three relevant entities.
The combined score across all entities determined the best technology.
As mentioned before, the framework comparison has been difficult since some entities are not available using certain technologies.
In the case of Dialogflow, two date-span entities are available, which increased the testing overhead.
Table \ref{tab:entity_extraction_recognition} shows the supported entites next to the technology to get an overview.

% intent classification
The second primary function of chatbots is the intent classification.
The test setup used the same training and test data for each technology to provide a fair environment.
The intent classification evaluation uses the same approach as the entity extraction evaluation.
The performance measurement relies on precision, recall, and f-score values.
As mentioned in Chapter \ref{chap:basics} Basics, the NLU services of all four frameworks provide a confidence value.
The average confidence score is another performance quality metric.
The confidence value is a response that represents how well the input fits an intent.
Hence, it is logical to include the confidence score in the evaluation.
It indicates how good the training worked for the frameworks and also if additional training data would have been necessary.
Furthermore, the standard deviation shows how consistent the chatbots' confidence scores were. 
As a footnote, a chatbot always returns a confidence score for each intent.
For each test, the correct intent's confidence score was listed no matter if it was identified correctly or not.
This allowed the calculation of the average standard deviation across an intent and as a whole.
A high average confidence score combined with a high f-score indicates that the chatbot classifies intents reliably and vice versa.
For the use-cases, English and German are potentially interesting as input language.
Hence, the intent classification tests include both.
The expectation for the German tests was that there is no significant difference to the English tests.
The performance expectation was that the German bot is on the same level as the English bot regarding entity extraction and intent classification.
Contrary to the expectation, each technology showed significant differences.
The intent performance increased noticeably while the entity extraction performance dropped for Rasa, Dialogflow, and Watson Assistant.
LUIS had no person entity for the German input language.
Dialogflow disappointed in the entity extraction department.
This came as a surprise as the entities are not that different for the two languages.
 
% implementation possible?
As \citet{singhbuilding} mentioned, the best evaluation criterion is if the underlying task can be achieved or not.
This evaluation criterion has been adopted for this thesis since it is the 
most relevant for a technology recommendation.
As a result, value for this evaluation, a binary boolean criterion was chosen.
Either it was possible to build a prototype that implemented the design, or it was impossible.
The evaluation did not include an opinion or how hard the implementation was.
LUIS is an NLU technology and was excluded from this test because it offers no dialog handling mechanism.
NLU technologies do not provide dialog handling in general. 
The Microsoft Bot Framework uses LUIS and offers dialog handling but was not used in this thesis and is therefore not part of the evaluation.
As expected, the implementation was possible with all three chatbot technologies.

% speech recognition
For future projects, speech recognition capabilities are on the wish-list of the 3 Banken IT.
Thus, the costs of the speech-to-text and text-to-speech services were compared based on the price.
These services enable voice interaction between man and machine.
All chatbot providers should provide such services to extend the functionality of the bot.
Rasa is the only tested framework that does not offer such services.
It is possible to use the speech services of other providers.
The cloud providers offer speech services at comparable prices.

% price comparison
A key factor for the use of chatbots is the price.
That the price plays an important role was mentioned in \citet{buiildChatbotsPython}, for instance.
The literature found and used for this thesis did not compare the prices of the available technologies.
The goal of the price comparison was to find out which technologies are cheap and which are expensive.
The goal was to find a cheap technology that performs well.
The costs of a chatbot have been calculated for different categories since the prices listed online were intransparent and hard to compare.
The first category was the price for the chatbot technologies, the second 
one is the price for hosting, and the third category was the costs
for speech-to-text and text-to-speech services.
The websites of Rasa and IBMs Watson Assistant do not list the prices for the enterprise edition. 
It is calculated individually for each customer.
To build a chatbot with Microsoft technologies LUIS and the Microsoft Bot Framework are combined.
The added price for both builds the actual price for the comparison.
IBM calculates the price for Watson Assistant per user per month.
Dialogflow, LUIS, and the Microsoft Bot Framework calculate the price based on the number of requests while Rasa is free by default.
The prices for the text-to-speech and speech-to-text services were comparable for Watson Assistant, Microsoft Speech Services, and Dialogflow.
Rasa did not offer text-to-speech and speech-to-text services at all.
The general expectation was that the frameworks cost roughly the same, but the prices for the chatbots were hard to compare or not comparable at all.
The speech services cost roughly the same.
All in all, there is no exceptionally cheap or expensive technology.

% portability 
Another evaluation criterion used for this thesis is the portability of software systems, as defined in the ISO 25010\cite{iso25010} standard.
The portability is interesting for a comparison of cloud frameworks with a  local chatbot technology like Rasa.
For the development of a chatbot, portability can play a crucial role.

%% local VS cloud
The only tested technology which is portable was Rasa. 
The portability of the development environment of the local technology Rasa in contrast to the cloud chatbot technologies was the main focus of the first portability evaluation.
The number of providers that allow a deployment was the second factor for the portability evaluation.
The main focus lay on cloud technologies since the expectation was
that they can only run in the environment of the provider and are 
not portable at all.
The third portability category chosen was how flexible the development
of chatbots is regarding the devices.
The cloud providers use simple web interfaces.
No special setup and no additional tools are required to start the development of a chatbot.
The local technology required a development environment and is inflexible in comparison.

%% migration
Another aspect of portability is the migration from one technology to another.
The migration from one technology to another is hard or impossible, which meets the expectation.
Every technology uses the JSON format to store data, which seems perfect at first.
The information of every bot is stored differently, making it impossible to copy the information from one bot to another.
To summarize, this makes it impossible to migrate from one framework to another in a fast and straightforward way since the format of the data is incompatible. 

% project setup
The project setup complexity plays a crucial role in the development of a chatbot.
The main focus of this test was Rasa since the setup complexity was interesting for the local technology.
That Rasa is at a disadvantage in this category was clear from the start.
Cloud chatbots require no setup at all since the provider takes care of everything.
The real question is how much harder is the local setup.
It was harder to set everything up for a Rasa project.
With Docker, the setup of Rasa was easy and fast.  

% deployment complexity
The deployment of chatbot systems is an important aspect of portability for local and cloud technologies.
Cloud chatbot technologies should not produce any deployment overhead at all.
They are hosted in the cloud by default, which makes a setup unnecessary.
For Rasa, the opposite was true in this category.
No cloud deployment is available by default, and the developer has to take care of it if required.

% Docker
This may seem like a grand disadvantage at first glance, but thanks to the Docker compatibility Rasa come with some huge advantages that cloud chatbots cannot offer.
Rasa can run in a Docker container.
This means it can run almost everywhere, while the cloud technologies are limited to the providers' cloud.
The deployment in cloud environments, on local machines, and internal servers is possible.

% learnability
The ISO 25010\cite{iso25010} defines learnability as a sub-criterion of usability as a measurement metric for software systems.
The learnability is important from a development perspective since a technology that is easy to learn and use is the optimal case.
The performance is still important and needs to be high nonetheless.
Evaluating the project setup and deployment complexity was also part of the learnability evaluation since it plays a role in the development process.

% predefined entities
The predefines entities can save lots of development time and are often of high quality.
As mentioned before, the chatbot technologies use entity extraction to extract data from text.
The creation of entities is a part of the development process of a chatbot, and it takes time to define good entities.
All of the chatbot technologies offer predefined entities that are ready to use.
As a criterion, the number of predefined entities was defined as a measure since lots of predefined entities can save lots of development time for future projects.
If a required entity is available for a given problem, the developer does not need to create one.
The second measurement criterion of the entity category is if the chatbot technology predefined all required entities for the use-cases of this thesis.
The technologies where all required entities were predefined are better suitable to solve the given problem.
Another aspect was how hard the creation of new intents, entities, training phrases, and dialogs was.
The expectation was that it is equally complicated for each technology. This proofed to be the case for cloud technologies.

%% design of conversations, dialog nodes, and stories
As mentioned by \citet{folstad2017chatbots}, the design of chatbots is all about the conversation.
Hence, the most important aspect was the creation of dialogs.
The frameworks Dialogflow and Watson Assistant offered dialog nodes for the creation and structuring of conversations.
A dialog node is one step of the conversation and optionally followed by other dialog nodes.
A dialog node was the classification of the sickness notification intent followed by the collection of the name and return date and the resulting response of the system, for instance.
Rasa offered stories to represent a dialog.
A story represents a whole conversation path from start to finish.
As an example, a story starts with a greeting phrase and ends with a goodbye message.

%% story vs dialog node
Rasa's stories were adjustable fine granular compared to the dialog node approach of Dialogflow and Watson Assistant.
A story can execute multiple actions and display multiple responses in a row.
Watson Assistant cannot execute multiple actions but can display multiple messages.
Dialogflow cannot execute multiple actions nor display multiple responses. 

%% form-data
The extraction of form-data is a required feature for this thesis.
It was necessary to extract such data from the conversation.
The main focus of the evaluation was the complexity of the form-data extraction.
The cloud bots provided the required extraction mechanisms out of the box. 
Rasa, on the other hand, was the only technology tested where programming skills were necessary to extract form data from the conversation. 
To conclude, the chatbots were easy to use in this category, while Rasa was not.

%% action server VS regular form data
Rasa's action server is more complicated to implement but also has advantages over the form-data approach of the cloud bots.
Programming skills were required to implement form-data extraction with the action server, while the cloud bots do not require any programming skills at all.
This means that the advantages of programming languages are available at the action server.
Data validation, the introduction of logic, and calling external services is easy in programming languages. 
The two approaches are quite different since the focus appears to be different.
The form-data extraction of the cloud bots is easy to use and very simple, while Rasa is more complicated but offers the advantages of a programming language. 

% customization possibilities
%% pipeline: custom components
Customization and fine-tuning of chatbot systems might be important for developers when the uses-cases have very specific requirements or need fine-tuning.
Rasa is the only technology that allows adjustments in the frameworks pipeline.
This allows a developer to replace components or add custom components to the pipeline and makes fine-tuning possible.
The pipeline of the other tested technologies is not even visible for developers.
Adjustments of Rasa's pipeline were necessary to get the required entities of external tools into the application.
Duckling and Spacy were added to the pipeline to get the three required entities as predefined entities.
Something like that is impossible with the other frameworks since the pipeline cannot be adjusted at all.
The pipeline also allows detailed adjustments of ML parameters, which is not the case for the cloud bots.
Developers can fine-tune the application by adjusting ML parameters, adding additional components, or replacing components.

% framework concepts
%% intents, entities, utterances
As discussed in Chapter \ref{chap:basics} Basics, the concepts used by the frameworks are interesting.
The base concepts intent, entity, and utterance are present in literature, and the expectation was that these concepts are also present in the frameworks.
This proofed to be the case for all four technologies.

% communication
Another important aspect is the communication with the chatbot services.
%% JSON format
In general, JSON is the request and response format for all tested technologies.
All the technologies offered a REST endpoint for communication.
%% meta info
The bots send different message content to external services.
LUIS and Dialogflow sent metadata while Watson Assistant and Rasa did not.
The evaluation focused on these two approaches since they are quite different and have positive and negative aspects.
The meta-information is valuable for the creation of statistics and performance tracking, for instance.
The other approach is simpler and has no predefined structure.
Hence, the developers can choose the format freely without any restrictions.
It is also possible to add the required meta-data to the chatbot message. 

% process external information
External services can send additional information to chatbots through the response messages.
The question was, how easy is the processing of the additional information using the chatbot framework.
The expectation was that it is easies with Rasa since the action server handles the response.
As a reminder, the action server uses Python, and it is easy to process data using a programming language.

% training capabilities
Another big difference between Rasa and the other technologies were the 
training capabilities.
%% live training
Live training is offered by Rasa while all the other technologies 
were solely trained with the training data defined by the developer.
With live training, the created stories should be closer 
to a real-life scenario because the developer has to go through the conversation step by step to create the story.
This can prevent unnatural feeling conversations, and it is also impossible to forget a step of the conversation.

% user interface design
In the ISO 25012\cite{iso25010} defines the user interface aesthetics under the section usability. 
Hence the user interfaces of the frameworks were also compared regarding simplicity.
%% how simple, how easy to orient
The evaluation of the UI is subjective and contains the author's opinion and objective, measurable values where possible.
The interface of a framework was evaluated as easy to use when the overall structuring is easy to understand, and the prototype was easy to implement.

\section{Course of Action} % Ablauf
The first step was the problem analyzation and the creation of the flow-charts for the dialogs.
Based on the flow-chart, the required intents and entities were defined.
The required intents are sickness notification and vacation request.
For each use-case, one intent was necessary.
The conversation built in the flow-chart made the required entities visible.
The sickness notification requires the name of a person and an optional return date.
The vacation request needed the name of the person and a date-span.
To summarize, the resulting entities were person, date, and date-span.
For each intent, ten training sentences in English and ten training sentences in German were created.
Additionally, test sentences for the validation of the result were created.
The entity extraction was tested with sentence fragments.

The prototypes used the training sentences for the creation of the intents.
The result of each test is either TP, FP, TN, or FN.
Based on these values, the precision, recall, and f-score values were calculated to provide an objective evaluation.
The same approach was used to evaluate the intent classification.
The confidence and standard deviation values were added for the intent classification to provide more evaluation factors.
For each test category, a summary compared the performance of the technology in the given category.
The frameworks evaluation is based on the criteria listed in Section \ref{sec:comparison_and_eval}.
The ranking of the frameworks uses the results in the categories to show strengths and weaknesses.
This made a final recommendation for further projects possible.
Furthermore, the entity extraction and intent classification result in English and German showed which technology performed best in the given domain and language.